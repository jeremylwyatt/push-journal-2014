
\section{Conclusions}\label{sec:Discussion}

This paper has made the following contributions to learning transferable models of object behaviour:

\noindent {\bf Modular predictors of object motion can be learned.} This is supported by experiment P1, where the behaviour of three real objects was learned, in a modular fashion, by eight of the nine learning approaches in Table~\ref{tab:algs}. 

\noindent {\bf Learning transfer can be achieved.} The simulated and real runs from experiments P2 and P3 both support the hypotheses H2  and H3 that learning transfer can be achieved with respect to action and shape. The analysis of simulated runs explains why some information-algorithm combinations succeed and some do not. 

\noindent {\bf Contact information assists transfer.} We hypothesised that modelling agent-object and object-environment contacts is important for learning transferable models. This hypothesis is supported by experiments P2 and P3. In simulation adding information G+A+E can give substantial performance improvements, for both transfer to novel actions (Figure~\ref{fig:A_av_graphs}) (left panel) and to novel shapes (Figure~\ref{fig:S_av_graphs}) (bottom left).  This result held for shape transfer for one of the two real object cases. 

%\begin{table}[b]
%\begin{center}
%\begin{tabular}{|l|l|l|l|l|}
%\cline{3-5}
%\multicolumn{2}{c}{ } & \multicolumn{3}{|c|}{Information Utilised} \\
%\cline{1-5}
%Predictor & data & Global\,(G) & G\,\&\,Agent\,(A) & G\,\&\,A\,\&\,Env \\
%\cline{1-5}
%KDEF & sim & 0.189$\pm$0.004 & 0.188$\pm$0.010 & \textbf{0.025}$\pm$0.001 \\
%LWPR & sim & 0.154$\pm$0.011 & 0.083$\pm$0.007 & 0.243$\pm$0.001 \\
%KDE & sim & n/a & 0.105$\pm$0.003 & 0.315$\pm$0.001 \\
%\cline{1-5}
%KDEF & real & 0.181$\pm$0.002 & 0.134$\pm$0.003 & \textbf{0.107}$\pm$0.002 \\
%LWPR & real & 0.200$\pm$0.002 & 0.252$\pm$0.003 & 0.231$\pm$0.002 \\
%KDE & real & n/a & 0.186$\pm$0.002 & 0.198$\pm$0.001 \\
%\cline{3-5}
%PhysX & real & \multicolumn{3}{|c|}{0.103$\pm$0.003} \\
%\cline{1-5}
%\end{tabular}
%\end{center}
%\caption[Performance Table]{Experiment S-transfer: Generalisation to novel shape.
%Trained on cylinder and box, tested on double-cylinder, for simulated and real data.
%Comparative performance of predictors vs. information used.
%Shown is the dimensionless measure normalised average error ${E_{av}^{norm}} \pm$ standard error.
%}\label{tab:PerformanceTableS2av}
%\end{table}

\noindent {\bf Factorisation assists transfer.} Modelling contacts is hard. The right representation is required to exploit the available information. Experiments P2 and P3 in simulation show that factoring the density estimation problem by contacts aids learning (see Figure~\ref{fig:A_av_graphs} (left panel), and Figure~\ref{fig:S_av_graphs} (bottom left panel and top right panel). This supports the hypothesis that factorisation of information by contact assists transfer. 
\newlength{\imgCXwid}
\setlength{\imgCXwid}{2.15cm}
\begin{figure*}[tbp]
%\centerline{
%\includegraphics[width=2.3cm]{images/C1_2exp_48_1}
%\includegraphics[width=2.3cm]{images/C1_2exp_48_2}
%\includegraphics[width=2.3cm]{images/C1_2exp_48_3}
%\includegraphics[width=2.3cm]{images/C1_2exp_48_4}
%\includegraphics[width=2.3cm]{images/C1_2exp_48_5}
%}
%\vspace{0.1cm}
\centerline{
\includegraphics[width=\imgCXwid]{images/C1_2exp_87_1}
\includegraphics[width=\imgCXwid]{images/C1_1exp_87_1}
\includegraphics[width=\imgCXwid]{images/C1_LWPR1_87_1}
\includegraphics[width=\imgCXwid]{images/C5_1exp_6_1}
\includegraphics[width=\imgCXwid]{images/C5_2exp_6_1}
\includegraphics[width=\imgCXwid]{images/C5_3exp_6_1}
\includegraphics[width=\imgCXwid]{images/C2_3exp_75_1}
}
%\vspace{0.1cm}
\centerline{
\includegraphics[width=\imgCXwid]{images/C1_2exp_87_2}
\includegraphics[width=\imgCXwid]{images/C1_1exp_87_2}
\includegraphics[width=\imgCXwid]{images/C1_LWPR1_87_2}
\includegraphics[width=\imgCXwid]{images/C5_1exp_6_2}
\includegraphics[width=\imgCXwid]{images/C5_2exp_6_2}
\includegraphics[width=\imgCXwid]{images/C5_3exp_6_2}
\includegraphics[width=\imgCXwid]{images/C2_3exp_75_2}
}
%\vspace{0.1cm}
\centerline{
\includegraphics[width=\imgCXwid]{images/C1_2exp_87_3}
\includegraphics[width=\imgCXwid]{images/C1_1exp_87_3}
\includegraphics[width=\imgCXwid]{images/C1_LWPR1_87_3}
\includegraphics[width=\imgCXwid]{images/C5_1exp_6_3}
\includegraphics[width=\imgCXwid]{images/C5_2exp_6_3}
\includegraphics[width=\imgCXwid]{images/C5_3exp_6_3}
\includegraphics[width=\imgCXwid]{images/C2_3exp_75_3}
}
\centerline{
\includegraphics[width=\imgCXwid]{images/C1_2exp_87_4}
\includegraphics[width=\imgCXwid]{images/C1_1exp_87_4}
\includegraphics[width=\imgCXwid]{images/C1_LWPR1_87_4}
\includegraphics[width=\imgCXwid]{images/C5_1exp_6_4}
\includegraphics[width=\imgCXwid]{images/C5_2exp_6_4}
\includegraphics[width=\imgCXwid]{images/C5_3exp_6_4}
\includegraphics[width=\imgCXwid]{images/C2_3exp_75_4}
}
%\vspace{0.1cm}
\centerline{
\includegraphics[width=\imgCXwid]{images/C1_2exp_87_5}
\includegraphics[width=\imgCXwid]{images/C1_1exp_87_5}
\includegraphics[width=\imgCXwid]{images/C1_LWPR1_87_5}
\includegraphics[width=\imgCXwid]{images/C5_1exp_6_5}
\includegraphics[width=\imgCXwid]{images/C5_2exp_6_5}
\includegraphics[width=\imgCXwid]{images/C5_3exp_6_5}
\includegraphics[width=\imgCXwid]{images/C2_3exp_75_5}
}

\caption {Experiment P3: Shape Transfer. Green outline shows predictions. Column~1: KDEF-GA/quat.
  Col~2: KDEF-G/quat. Col~3: LWPR-G for one trial.  Note that the
  KDEF-G/quat and LWPR-G methods predict that the robot finger moves
  into the box.  Col~4: KDEF-G/quat. Col~5: KDEF-GA/quat. Col~6:
  KDEF-GAE/quat. Col~7: KDEF-GAE/quat. The frame number is shown in
  the top left of each image.  }
\label{fig:ExperimentStransfer}
\end{figure*}

\noindent {\bf Learning can match or exceed physics engine performance.} In experiment P1, in 23 of 24 cases the learners significantly outperformed a tuned physics engine. The learned predictions were usually physically plausible (see Figures~\ref{fig:ExperimentL2}). This supports hypothesis H1. In experiments P2 and P3 learning transfer using factored KDE was able to match or improve on the prediction error of a physics engine (see Figures~\ref{fig:A_av_graphs} right panel and Figure~\ref{fig:S_av_graphs} top row). Its predictions were also usually physically plausible (see Figure~\ref{fig:ExperimentA} columns 4 and 7, and Figure~\ref{fig:ExperimentStransfer} columns 1,6 and 7).

\noindent {\bf Limitations and extensions:} This work is a first attempt to perform transfer learning for motion models of objects. There are two limitations to the methods presented. The most important is seen in the degradation of transfer performance in the face of observation noise. Recently we have extended a non-factored form of our learning algorithm to remove noise at prediction time using kinematic optimisation \cite{belter2014iros}, this is a promising route to improving transfer performance given noisy training data. The second desirable extension is that at the moment learning and transfer both require selection of the attachment points of frames to each body. This process needs to be automated.

This paper establishes that predictions of object behaviour can be learned using a variety of techniques; that they can outperform physics engine predictions; that they can generalise interpolatively to similar actions and shapes; that they can be transfered to novel actions and shapes, and that they produce physically plausible predictions. Transfer learning of such models, while challenging, is possible with the right representation.  In particular a factored problem formulation encodes transferable predictions that are physically plausible and qualitatively correct.